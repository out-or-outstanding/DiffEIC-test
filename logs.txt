/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.
  warning_cache.warn(
Validation sanity check: 100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [01:36<00:00, 96.40s/it]/workspace/project/DiffEIC/model/diffeic.py:829: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  outputs = np.array(outputs)
Global seed set to 231                                                                                                                         
/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:393: UserWarning: The number of training samples (2) is smaller than the logging interval Trainer(log_every_n_steps=100). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Epoch 0:   0%|                                                                                                           | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):
  File "test/train_model_with_bg_text.py", line 49, in <module>
    main()
  File "test/train_model_with_bg_text.py", line 45, in main
    trainer.fit(model, datamodule=data_module)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in fit
    self._call_and_handle_interrupt(
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 682, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 770, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1193, in _run
    self._dispatch()
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1272, in _dispatch
    self.training_type_plugin.start_training(self)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 202, in start_training
    self._results = trainer.run_stage()
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1282, in run_stage
    return self._run_train()
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1312, in _run_train
    self.fit_loop.run()
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 234, in advance
    self.epoch_loop.run(data_fetcher)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 195, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 145, in run
    self.advance(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 215, in advance
    result = self._run_optimization(
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 266, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 378, in _optimizer_step
    lightning_module.optimizer_step(
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py", line 1662, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 164, in step
    trainer.accelerator.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 336, in optimizer_step
    self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 163, in optimizer_step
    optimizer.step(closure=closure, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/optim/adamw.py", line 148, in step
    loss = closure()
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py", line 148, in _wrap_closure
    closure_result = closure()
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 160, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 142, in closure
    step_output = self._step_fn()
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 435, in _training_step
    training_step_output = self.trainer.accelerator.training_step(step_kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 216, in training_step
    return self.training_type_plugin.training_step(*step_kwargs.values())
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py", line 441, in training_step
    return self.model(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py", line 81, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/workspace/project/DiffEIC/model/diffeic.py", line 782, in training_step
    loss, loss_dict = self.shared_step(batch)
  File "/workspace/project/DiffEIC/ldm/models/diffusion/ddpm.py", line 857, in shared_step
    loss = self(x, c)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/project/DiffEIC/ldm/models/diffusion/ddpm.py", line 869, in forward
    return self.p_losses(x, c, t, *args, **kwargs)
  File "/workspace/project/DiffEIC/model/diffeic_with_txt_background.py", line 566, in p_losses
    model_output = self.apply_model(x_noisy, t, cond)
  File "/workspace/project/DiffEIC/model/diffeic_with_txt_background.py", line 444, in apply_model
    z_fused = self.cmf(x_noisy, z_bg_corrected, z_txt)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/project/DiffEIC/model/diffeic_with_txt_background.py", line 45, in checkpoint_forward
    return torch.utils.checkpoint.checkpoint(
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 249, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/opt/conda/envs/diffeic/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 107, in forward
    outputs = run_function(*args)
  File "/workspace/project/DiffEIC/model/diffeic_with_txt_background.py", line 226, in forward
    z_fused = α * attn_img + β * (z_bg_aligned + attn_bg) + γ * (z_txt_aligned + attn_txt)
RuntimeError: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 3